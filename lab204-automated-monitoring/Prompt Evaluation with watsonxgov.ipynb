{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Prompts/Prompt Template Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run using with Runtime 22.2 & Python 3.10 or greater runtime environment. If you are viewing this in Watson Studio and do not see Python 3.10.x in the upper right corner of your screen, please update the runtime now. \n",
    "\n",
    "The notebook will create a summarization prompt template asset in a given project, configure OpenScale to monitor that PTA and evaluate generative quality metrics and model health metrics. The notebook then promotes the prompt template asset to space and does the same evaluation.\n",
    "\n",
    "If users wish to execute this notebook for task types other than summarization, please consult [this](https://github.com/IBM/watson-openscale-samples/blob/main/IBM%20Cloud/WML/notebooks/watsonx/README.md) document for guidance on evaluating prompt templates for the available task types.\n",
    "\n",
    "Note : User can search for `EDIT THIS` and fill the inputs needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It requires service credentials for IBM Watson OpenScale:\n",
    "* Requires a CSV file containing the test data that needs to be evaluated\n",
    "* Requires the ID of project in which you want to create the prompt template asset.\n",
    "* Requires the ID of space to which you want to promote the prompt template asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Setup](#settingup)\n",
    "- [Create Prompt template](#prompt)\n",
    "- [Prompt Setup](#ptatsetup)\n",
    "- [Risk evaluations for prompt template asset subscription](#evaluate)\n",
    "- [Display the Model Risk metrics](#mrmmetric)\n",
    "- [Display the Generative AI Quality metrics](#genaimetrics)\n",
    "- [See factsheets information](#factsheetsspace)\n",
    "- [Evaluate prompt template from space](#evaluatespace)\n",
    "- [Promote prompt template asset to space](#promottospace)\n",
    "- [Create deployment for prompt template asset in space](#ptadeployment)\n",
    "- [Setup the prompt template asset in space](#ptaspace)\n",
    "- [Score the model and configure monitors](#score)\n",
    "- [See factsheets information from space](#factsheetsproject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=\"settingup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision services and configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume here that you are reusing the watsonx.governance instance (and other services related to it) for the code shown below. If you are running the notebook outside the scope of an actual class, you will have to provision an instance if you don't already have one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Cloud API key can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below.\n",
    "\n",
    "**Make sure that you are in the right IBM Cloud account when creating this API key. If you create it in your personal account, it will most likely not work with the watsonx.governance instance used for this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You can also get an `API_KEY` using the IBM CLOUD CLI.\n",
    "\n",
    "How to install IBM Cloud (bluemix) console: [instruction](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli.html#install_use)\n",
    "\n",
    "How to get api key using console:\n",
    "```\n",
    "ibmcloud login --sso\n",
    "ibmcloud iam api-key-create 'my_key'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAM_URL=\"https://iam.cloud.ibm.com/identity/token\"\n",
    "DATAPLATFORM_URL = \"https://api.dataplatform.cloud.ibm.com\"\n",
    "CLOUD_API_KEY = \"\" # YOUR_CLOUD_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "                   \"apikey\": CLOUD_API_KEY\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read project id from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set up a development type subscription, the PTA must be within the project. Please supply the project ID where the PTA needs to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"\" #YOUR_PROJECT_ID  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read space id from user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use an existing space or can create a new space to promote the model, by setting the below variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_existing_space = True # Set it as False if user wants to create a new space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "wml_client = APIClient(WML_CREDENTIALS)\n",
    "wml_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below details are required if user choose to use an existing space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use an existing space, you can directly add the space id in the below cell. If you set the `use_existing_space` variable above to True, you have to specify a value here, otherwise you can leave it empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.spaces.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_space_id = \"\" # YOUR_SPACE_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_existing_space == True:\n",
    "    space_id = existing_space_id\n",
    "else:\n",
    "    space_id = wml_client.spaces.store(\n",
    "        meta_props={wml_client.spaces.ConfigurationMetaNames.NAME: space_name,\n",
    "                   wml_client.spaces.ConfigurationMetaNames.STORAGE: {\"resource_crn\":COS_RESOURCE_CRN},\n",
    "                   wml_client.spaces.ConfigurationMetaNames.COMPUTE: {\"name\": WML_INSTANCE_NAME, \"crn\": WML_CRN},\n",
    "                   wml_client.spaces.ConfigurationMetaNames.TYPE: \"wx\"\n",
    "                   })[\"metadata\"][\"id\"]\n",
    "wml_client.set.default_space(space_id)\n",
    "print(space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create the access token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates an IAM access token using the provided credentials. The API calls for creating and scoring prompt template assets utilize the token generated by this function.\n",
    "Note that the token is only valid for an hour, after that time you will have to generate a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "def generate_access_token():\n",
    "    headers={}\n",
    "    headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n",
    "    headers[\"Accept\"] = \"application/json\"\n",
    "    data = {\n",
    "        \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n",
    "        \"apikey\": CLOUD_API_KEY,\n",
    "        \"response_type\": \"cloud_iam\"\n",
    "    }\n",
    "    response = requests.post(IAM_URL, data=data, headers=headers)\n",
    "    json_data = response.json()\n",
    "    iam_access_token = json_data['access_token']\n",
    "    return iam_access_token\n",
    "\n",
    "iam_access_token = generate_access_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompt template <a name=\"prompt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a prompt template for a summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Payload to create the PTA\n",
    "payload = {\n",
    "    \"name\": \"Summarize input\",\n",
    "    \"description\": \"Summarize the given content\",\n",
    "    \"task_ids\": [\n",
    "        \"summarization\"\n",
    "    ],\n",
    "    \"prompt_variables\": {\n",
    "        \"original_text\": {}\n",
    "    },\n",
    "    \"prompt\": {\n",
    "        \"input\": [\n",
    "            [\n",
    "                \"summarize the given content {original_text}\",\n",
    "                \"\"\n",
    "            ]\n",
    "        ],\n",
    "        \"data\": {\n",
    "            \"instruction\": \"summarize the given content {original_text}\"\n",
    "        },\n",
    "        \"model_id\": \"google/flan-ul2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "params = {\"project_id\": project_id}\n",
    "\n",
    "url = DATAPLATFORM_URL + \"/wx/v1-beta/prompts\"\n",
    "headers={}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"*/*\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(iam_access_token)\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers, params = params)\n",
    "json_data = response.json()\n",
    "project_pta_id = json_data[\"id\"]\n",
    "project_pta_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt setup <a name=\"ptatsetup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure OpenScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "\n",
    "service_instance_id = None # Update this to refer to a particular service instance\n",
    "authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY, url = \"https://iam.cloud.ibm.com\")\n",
    "wos_client = APIClient(authenticator=authenticator, service_url = \"https://aiopenscale.cloud.ibm.com\", service_instance_id = service_instance_id)\n",
    "wos_client.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the prompt template asset in project for evaluation with supported monitor dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template assets from project is only supported with `development` operational space ID. Running the below cell will create a development type subscription from the prompt template asset created within the project.\n",
    "\n",
    "The available parameters that can be passed for `execute_prompt_setup` function are:\n",
    "\n",
    " * `prompt_template_asset_id` : Id of prompt template asset for which subscription needs to be created.\n",
    " * `label_column` :  The name of the column containing the ground truth or actual labels.\n",
    " * `project_id` : The GUID of the project.\n",
    " * `space_id` : The GUID of the space.\n",
    " * `deployment_id` : (optional) The GUID of the deployment.\n",
    " * `operational_space_id` : The rank of the environment in which the monitoring is happening. Accepted values are `development`, `pre_production`, `production`.\n",
    " * `problem_type` : (optional) The task type to monitor for the given prompt template asset.\n",
    " * `classification_type` : The classification type `binary`/`multiclass` applicable only for `classification` problem (task) type.\n",
    " * `input_data_type` : The input data type.\n",
    " * `supporting_monitors` : Monitor configuration for the subscription to be created.\n",
    " * `background_mode` : When `True`, the promt setup operation will be executed in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"reference_summary\"\n",
    "operational_space_id = \"development\"\n",
    "problem_type= \"summarization\"\n",
    "input_data_type= \"unstructured_text\"\n",
    "\n",
    "\n",
    "monitors = {\n",
    "    \"generative_ai_quality\": {\n",
    "        \"parameters\": {\n",
    "\n",
    "            \"min_sample_size\": 10,\n",
    "            \"metrics_configuration\":{\n",
    "                \n",
    "            \"bleu\": {\n",
    "                \"max_order\": 4,\n",
    "                \"smooth\": \"false\"\n",
    "            },\n",
    "            \"cosine_similarity\": {},\n",
    "            \"hap_score\": {\n",
    "                \"record_level_max_score\": 0.5\n",
    "            },\n",
    "            \"jaccard_similarity\": {},\n",
    "            \"meteor\": {\n",
    "                \"alpha\": 0.9,\n",
    "                \"beta\": 3,\n",
    "                \"gamma\": 0.5\n",
    "            },\n",
    "            \"normalized_f1\": {},\n",
    "            \"normalized_precision\": {},\n",
    "            \"normalized_recall\": {},\n",
    "            \"rouge_score\": {\n",
    "                \"use_aggregator\": \"true\",\n",
    "                \"use_stemmer\": \"true\"\n",
    "            },\n",
    "            \"sari\": {},\n",
    "            \"pii\": {\n",
    "              \"record_level_max_score\": 0.5\n",
    "             }\n",
    "                    \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = wos_client.monitor_instances.mrm.execute_prompt_setup(prompt_template_asset_id = project_pta_id, \n",
    "                                                                   project_id = project_id,\n",
    "                                                                   label_column = label_column, \n",
    "                                                                   operational_space_id = operational_space_id, \n",
    "                                                                   problem_type = problem_type,\n",
    "                                                                   input_data_type = input_data_type, \n",
    "                                                                   supporting_monitors = monitors, \n",
    "                                                                   background_mode = False)\n",
    "\n",
    "result = response.result\n",
    "result._to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the below cell, users can  read the  prompt setup task and check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = wos_client.monitor_instances.mrm.get_prompt_setup(prompt_template_asset_id = project_pta_id,\n",
    "                                                             project_id = project_id)\n",
    "\n",
    "result = response.result\n",
    "result_json = result._to_dict()\n",
    "\n",
    "if result_json[\"status\"][\"state\"] == \"FINISHED\":\n",
    "    print(\"Finished prompt setup : The response is {}\".format(result_json))\n",
    "else:\n",
    "    print(\"prompt setup failed The response is {}\".format(result_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subscription id from prompt setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once prompt setup status is finished, Read the subscription id from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_subscription_id = result_json[\"subscription_id\"]\n",
    "dev_subscription_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all the monitor instances of the production subscription\n",
    "The following cell lists the monitors present in the development subscription along with their respective statuses and other details. Please wait for all the monitors to be in active state before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show(target_target_id = dev_subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data_mart_id of the monitor instances\n",
    "\n",
    "All monitor instances are associated with the same datamart, whose id is the first column in the table that's outputted after running the code above. Copy and paste the id and assign it to variable `data_mart_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mart_id = \"\" # YOUR_DATAMART_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk evaluations for PTA subscription <a name=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prompt template subscription\n",
    "\n",
    "For the risk assessment of a development type subscription the user needs to have an evaluation dataset. The risk evaluation function takes the evaluation dataset path as a parameter for evaluation of the configured metric dimensions. If there is a discrepancy between the feature columns in the subscription and the column names in the uploading CSV, users has the option to supply a mapping JSON file to associate the CSV column names with the feature column names in the subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"summarization.csv\"\n",
    "body = None # Please update your mapping file path here if needed\n",
    "\n",
    "# Download feedback data from project to local directory\n",
    "# Run the below code snippet only if you are running the notebook via watson studio\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "wslib = access_project_or_space({\"token\":iam_access_token})\n",
    "wslib.download_file(test_data_path)\n",
    "if body:\n",
    "    wslib.download_file(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the MRM monitor instance id\n",
    "\n",
    "Evaluating the test data against the prompt template subscription requires the monitor instance ID of MRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"mrm\"\n",
    "target_target_id = dev_subscription_id\n",
    "result = wos_client.monitor_instances.list(data_mart_id=data_mart_id,\n",
    "                                           monitor_definition_id=monitor_definition_id,\n",
    "                                           target_target_id=target_target_id,\n",
    "                                           project_id=project_id).result\n",
    "result_json = result._to_dict()\n",
    "mrm_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "mrm_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will assess the test data with the subscription of the prompt template asset and produce relevant measurements for the configured monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set_name = \"data\"\n",
    "content_type = \"multipart/form-data\"\n",
    "\n",
    "response  = wos_client.monitor_instances.mrm.evaluate_risk(monitor_instance_id=mrm_monitor_id, \n",
    "                                                    test_data_set_name = test_data_set_name, \n",
    "                                                    test_data_path = test_data_path,\n",
    "                                                    content_type = content_type,\n",
    "                                                    body = body,\n",
    "                                                    project_id = project_id,\n",
    "                                                    background_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the risk evaluation response\n",
    "\n",
    "After initiating the risk evaluation, the evaluation results are now available for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_id, project_id = project_id)\n",
    "response.result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the Model Risk metrics <a name=\"mrmmetric\"></a>\n",
    "\n",
    "Having calculated the measurements for the Foundation Model subscription, the MRM metrics generated for this subscription are now available for your review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_id, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the Generative AI Quality metrics <a name=\"genaimetrics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Monitor instance ID of Generative ai quality metrics is required for reading its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"generative_ai_quality\"\n",
    "result = wos_client.monitor_instances.list(data_mart_id = data_mart_id,\n",
    "                                           monitor_definition_id = monitor_definition_id,\n",
    "                                           target_target_id = target_target_id,\n",
    "                                           project_id = project_id).result\n",
    "result_json = result._to_dict()\n",
    "genaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "genaiquality_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the GenAIQ monitor metrics generated through the risk evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See factsheets information <a name=\"factsheetsspace\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factsheets_url = \"https://dataplatform.cloud.ibm.com/wx/prompt-details/{}/factsheet?context=wx&project_id={}\".format(project_pta_id, project_id)\n",
    "print(\"User can navigate to the published facts in project {}\".format(factsheets_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Prompt template from space <a name=\"evaluatespace\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick recap of what we have done so far.\n",
    "\n",
    "1. We've created a prompt template asset in project.\n",
    "2. We've created a `development` type subscription of prompt template asset in OpenScale.\n",
    "3. Configured monitors supported by OpenScale for the subscriptions.\n",
    "4. We've performed risk evaluations against the PTA susbscription with a sample set of test data.\n",
    "5. Displayed the metrics generated with the risk evaluation.\n",
    "6. Displayed the factsheets information for the subscription.\n",
    "\n",
    "Now, we can promote the created prompt template asset to space and perform similar actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promote PTA to space <a name=\"promottospace\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell promotes the prompt template asset from the project to the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"{}/v2/assets/{}/promote\".format(DATAPLATFORM_URL ,project_pta_id)\n",
    "\n",
    "params = {\n",
    "    \"project_id\":project_id\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"space_id\": space_id\n",
    "}\n",
    "response = requests.post(url, json=payload, headers=headers, params = params)\n",
    "json_data = response.json()\n",
    "space_pta_id = json_data[\"metadata\"][\"asset_id\"]\n",
    "space_pta_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create deployment for prompt template asset in space <a name=\"ptadeployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a subscription from space, it is necessary to create a deployment for prompt template assets in spaces. Note: If you get the error that the serving name already exists, create a new unique name and try again.\n",
    "\n",
    "**NOTE:** If you get the error that the serving name already exists, change `serving_name` to something different, like *summary_firstname_lastinitial*, *summary_firstinitial_lastinitial*, *summary_4*, *summary_5*, or *summary_6*, etc. until the serving name is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENTS_URL = WML_CREDENTIALS[\"url\"] + \"/ml/v4/deployments\"\n",
    "\n",
    "serving_name = \"summary_3\" # eg: summary_deployment\n",
    "\n",
    "payload = {\n",
    "    \"prompt_template\": {\n",
    "      \"id\": space_pta_id\n",
    "    },\n",
    "    \"online\": {\n",
    "       \"parameters\": {\n",
    "         \"serving_name\": serving_name\n",
    "       }\n",
    "    },\n",
    "    \"base_model_id\": \"google/flan-ul2\",\n",
    "    \"description\": \"summarization deployment\",\n",
    "    \"name\": \"summarization deployment\",\n",
    "    \"space_id\": space_id\n",
    "}\n",
    "\n",
    "version = \"2023-07-07\" # The version date for the API of the form YYYY-MM-DD. Example : 2023-07-07\n",
    "params = {\n",
    "    \"version\":version,\n",
    "    \"space_id\":space_id\n",
    "}\n",
    "\n",
    "response = requests.post(DEPLOYMENTS_URL, json=payload, headers=headers, params = params)\n",
    "json_data = response.json()\n",
    "\n",
    "\n",
    "if \"metadata\" in json_data:\n",
    "    deployment_id = json_data[\"metadata\"][\"id\"]\n",
    "    print(deployment_id)\n",
    "else:\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the prompt template asset in space for evaluation with supported monitor dimensions <a name=\"ptaspace\"></a>\n",
    "\n",
    "The prompt template assets from space is only supported with `pre_production` and `production` operational space IDs. Running the below cell will create a `production` type subscription from the prompt template asset promoted to the space. The `problem_type` value should depend on the task type specified in the prompt template asset.\n",
    "\n",
    "**Note:** Make sure you successfully obtained the `deployment_id` from the previous cell for this code to work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"reference_summary\"\n",
    "operational_space_id = \"production\"\n",
    "problem_type= \"summarization\"\n",
    "input_data_type= \"unstructured_text\"\n",
    "\n",
    "monitors = {\n",
    "    \"generative_ai_quality\": {\n",
    "        \"parameters\": {\n",
    "\n",
    "            \"min_sample_size\": 10,\n",
    "            \"metrics_configuration\":{\n",
    "                \n",
    "            \"bleu\": {\n",
    "                \"max_order\": 4,\n",
    "                \"smooth\": \"false\"\n",
    "            },\n",
    "            \"cosine_similarity\": {},\n",
    "            \"hap_score\": {\n",
    "                \"record_level_max_score\": 0.5\n",
    "            },\n",
    "            \"jaccard_similarity\": {},\n",
    "            \"meteor\": {\n",
    "                \"alpha\": 0.9,\n",
    "                \"beta\": 3,\n",
    "                \"gamma\": 0.5\n",
    "            },\n",
    "            \"normalized_f1\": {},\n",
    "            \"normalized_precision\": {},\n",
    "            \"normalized_recall\": {},\n",
    "            \"rouge_score\": {\n",
    "                \"use_aggregator\": \"true\",\n",
    "                \"use_stemmer\": \"true\"\n",
    "            },\n",
    "            \"sari\": {},\n",
    "            \"pii\": {\n",
    "              \"record_level_max_score\": 0.5\n",
    "             }\n",
    "                    \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"drift_v2\": {\n",
    "        \"thresholds\": [\n",
    "            {\n",
    "                \"metric_id\": \"confidence_drift_score\",\n",
    "                \"type\": \"upper_limit\",\n",
    "                \"value\": 0.05\n",
    "            },\n",
    "            {\n",
    "                \"metric_id\": \"prediction_drift_score\",\n",
    "                \"type\": \"upper_limit\",\n",
    "                \"value\": 0.05\n",
    "            },\n",
    "            {\n",
    "                \"metric_id\": \"input_metadata_drift_score\",\n",
    "                \"specific_values\": [\n",
    "                    {\n",
    "                        \"applies_to\": [\n",
    "                            {\n",
    "                                \"type\": \"tag\",\n",
    "                                \"value\": \"subscription\",\n",
    "                                \"key\": \"field_type\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"value\": 0.05\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"upper_limit\"\n",
    "            },\n",
    "            {\n",
    "                \"metric_id\": \"output_metadata_drift_score\",\n",
    "                \"specific_values\": [\n",
    "                    {\n",
    "                        \"applies_to\": [\n",
    "                            {\n",
    "                                \"type\": \"tag\",\n",
    "                                \"value\": \"subscription\",\n",
    "                                \"key\": \"field_type\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"value\": 0.05\n",
    "                    }\n",
    "                ],\n",
    "                \"type\": \"upper_limit\"\n",
    "            }\n",
    "        ],\n",
    "        \"parameters\": {\n",
    "            \"min_samples\": 10,\n",
    "            \"train_archive\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = wos_client.monitor_instances.mrm.execute_prompt_setup(prompt_template_asset_id = space_pta_id, \n",
    "                                                                   space_id = space_id,\n",
    "                                                                   deployment_id = deployment_id,\n",
    "                                                                   label_column = label_column, \n",
    "                                                                   operational_space_id = operational_space_id, \n",
    "                                                                   problem_type = problem_type,\n",
    "                                                                   input_data_type = input_data_type, \n",
    "                                                                   supporting_monitors = monitors, \n",
    "                                                                   background_mode = False)\n",
    "\n",
    "result = response.result\n",
    "result._to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the below cell, users can read the prompt setup task and check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = wos_client.monitor_instances.mrm.get_prompt_setup(prompt_template_asset_id = space_pta_id,\n",
    "                                                             deployment_id = deployment_id,\n",
    "                                                             space_id = space_id)\n",
    "\n",
    "result = response.result\n",
    "result_json = result._to_dict()\n",
    "result_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subscription id from prompt setup\n",
    "\n",
    "Once prompt setup status is finished, Read the subscription id from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_subscription_id = result_json[\"subscription_id\"]\n",
    "prod_subscription_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the PTA deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the scoring URL of the deployment from the subscription details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_details = wos_client.subscriptions.get(prod_subscription_id).result\n",
    "sub_details = sub_details._to_dict()\n",
    "scoring_url = sub_details[\"entity\"][\"deployment\"][\"url\"]\n",
    "if not scoring_url.find(\"?version=\") != -1:\n",
    "    scoring_url = scoring_url.strip() + \"?version=2023-09-07\"\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the model so that we can configure monitors <a name=\"score\"></a>\n",
    "\n",
    "Now that the WML service has been bound and the subscription has been created, we need to score the prompt template asset. User needs to generate the test data content in JSON format from the downloaded CSV file. This is used to construct the payload for scoring the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "feature_fields = [\"original_text\"]\n",
    "prediction = \"generated_text\"\n",
    "\n",
    "headers={}\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "headers[\"Accept\"] = \"*/*\"\n",
    "headers[\"Authorization\"] = \"Bearer {}\".format(iam_access_token)\n",
    "\n",
    "pl_data = []\n",
    "prediction_list = []\n",
    "with open(test_data_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        request = {\n",
    "            \"parameters\": {\n",
    "                \"template_variables\": {\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for each in feature_fields:\n",
    "            request[\"parameters\"][\"template_variables\"][each] = str(row[each])\n",
    "\n",
    "        response = requests.post(scoring_url, json=request, headers=headers).json()\n",
    "        predicted_val = response[\"results\"][0][prediction]\n",
    "        prediction_list.append(predicted_val)\n",
    "        record = {\"request\":request, \"response\":response}\n",
    "        pl_data.append(record)\n",
    "    \n",
    "pl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating additional payload data to enable drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable drift there should be minimum 100 records in the payload table. The below cell duplicates the scored records and create another 100 records for adding to the payload table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "additional_pl_data = copy.copy(pl_data)\n",
    "additional_pl_data *= 10\n",
    "print(\"Generated {} additional payload data\".format(len(additional_pl_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding payload data\n",
    "\n",
    "Below cell reads the payload data set id from the subscription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "\n",
    "time.sleep(5)\n",
    "payload_data_set_id = None\n",
    "payload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n",
    "                                                target_target_id=prod_subscription_id, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
    "if payload_data_set_id is None:\n",
    "    print(\"Payload data set not found. Please check subscription status.\")\n",
    "else:\n",
    "    print(\"Payload data set id: \", payload_data_set_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional payload data to enable drift V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=additional_pl_data,background_mode=False)\n",
    "time.sleep(5)\n",
    "pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a total of 110 records should be available within the payload table. But in case if auto payload logging fails to transmit the scored records to the payload logging table, the following code can be used to manually add payload data to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from ibm_watson_openscale.supporting_classes.payload_record import PayloadRecord\n",
    "time.sleep(5)\n",
    "pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "print(\"Number of records in the payload logging table: {}\".format(pl_records_count))\n",
    "if pl_records_count < 110:\n",
    "    print(\"Payload logging did not happen, performing explicit payload logging.\")\n",
    "    wos_client.data_sets.store_records(data_set_id=payload_data_set_id, request_body=pl_data,background_mode=False)\n",
    "    time.sleep(5)\n",
    "    pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n",
    "    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding feedback data\n",
    "\n",
    "Below cell reads the feedback data set id from the subscription. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "\n",
    "time.sleep(5)\n",
    "feedback_data_set_id = None\n",
    "feedback_data_set_id = wos_client.data_sets.list(type=DataSetTypes.FEEDBACK, \n",
    "                                                target_target_id=prod_subscription_id, \n",
    "                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n",
    "if feedback_data_set_id is None:\n",
    "    print(\"Feedback data set not found. Please check subscription status.\")\n",
    "else:\n",
    "    print(\"Feedback data set id: \", feedback_data_set_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code generates feedback data based on the downloaded CSV file and the scored response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_data_content = []\n",
    "csv_file_path = \"summarization.csv\"\n",
    "\n",
    "with open(csv_file_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row, prediction_val in zip(csv_reader, prediction_list):\n",
    "\n",
    "        # Read each row from the CSV and add label and prediction values\n",
    "        result_row = []\n",
    "        result_row = [row[key] for key in feature_fields if key in row]\n",
    "        result_row.append(row[label_column])\n",
    "        result_row.append(prediction_val)\n",
    "\n",
    "        test_data_content.append(result_row)\n",
    "if len(test_data_content) == 10:\n",
    "    print(\"generated feedback data from CSV\")\n",
    "else:\n",
    "    print(\"Failed to generated feedback data from CSV, Kindly verify the CSV file content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = feature_fields\n",
    "fields.append(label_column)\n",
    "fields.append(\"_original_prediction\")\n",
    "feedback_data = [\n",
    "    {\n",
    "        \"fields\": fields,\n",
    "        \"values\": test_data_content\n",
    "    }\n",
    "]\n",
    "feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to manually add feedback data to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.data_sets.store_records(data_set_id=feedback_data_set_id, request_body=feedback_data,background_mode=False)\n",
    "time.sleep(5)\n",
    "fb_records_count = wos_client.data_sets.get_records_count(feedback_data_set_id)\n",
    "# Adding time delay to enable drift\n",
    "time.sleep(10)\n",
    "print(\"Number of records in the feedback logging table: {}\".format(fb_records_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all the monitor instances of the production subscription\n",
    "The following cell lists the monitors present in the production subscription along with their respective statuses and other details. Please wait for all the monitors to be in active state before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show(target_target_id = prod_subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the MRM monitor instance ID of PTA subscription deployed in space\n",
    "\n",
    "Evaluating the test data against the prompt template subscription requires the monitor instance ID of MRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"mrm\"\n",
    "target_target_id = prod_subscription_id\n",
    "result = wos_client.monitor_instances.list(data_mart_id=data_mart_id,\n",
    "                                           monitor_definition_id=monitor_definition_id,\n",
    "                                           target_target_id=target_target_id,\n",
    "                                           space_id=space_id).result\n",
    "result_json = result._to_dict()\n",
    "mrm_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "mrm_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prompt template subscription from space\n",
    "\n",
    "The following cell will assess subscription of the prompt template asset and produce relevant measurements for the configured monitor. The data to be evaluated are already uploaded to payload and feedback table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response  = wos_client.monitor_instances.mrm.evaluate_risk(monitor_instance_id=mrm_monitor_id, \n",
    "                                                    body = body,\n",
    "                                                    space_id = space_id,\n",
    "                                                    evaluation_tests = [\"model_health\", \"drift_v2\", \"generative_ai_quality\"],\n",
    "                                                    background_mode = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the risk evaluation response\n",
    "\n",
    "After initiating the risk evaluation, the evaluation results of PTA from space are now available for review,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_id, space_id = space_id)\n",
    "response.result.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Model Risk metrics\n",
    "\n",
    "Having calculated the measurements for the Foundation Model subscription, the MRM metrics generated for this subscription are now available for your review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_id, space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Generative AI Quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Generative ai quality monitor instance id\n",
    "\n",
    "Monitor instance ID of Generative ai quality metrics is required for reading its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"generative_ai_quality\"\n",
    "result = wos_client.monitor_instances.list(data_mart_id = data_mart_id,\n",
    "                                           monitor_definition_id = monitor_definition_id,\n",
    "                                           target_target_id = target_target_id,\n",
    "                                           space_id = space_id).result\n",
    "result_json = result._to_dict()\n",
    "genaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "genaiquality_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the monitor metrics of GenAIQ generated through the risk evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Drift V2 metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Drift V2 monitor instance id\n",
    "\n",
    "Monitor instance ID of Drift V2 metrics is required for reading its metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_definition_id = \"drift_v2\"\n",
    "result = wos_client.monitor_instances.list(data_mart_id = data_mart_id,\n",
    "                                           monitor_definition_id = monitor_definition_id,\n",
    "                                           target_target_id = target_target_id,\n",
    "                                           space_id = space_id).result\n",
    "result_json = result._to_dict()\n",
    "drift_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n",
    "drift_monitor_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the monitor metrics of Drift V2 generated through the risk evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_client.monitor_instances.show_metrics(monitor_instance_id=drift_monitor_id, space_id=space_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User can navigate to see the published facts in space <a name=\"factsheetsproject\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factsheets_url = \"https://dataplatform.cloud.ibm.com/ml-runtime/deployments/{}/details?space_id={}&context=wx&flush=true\".format(deployment_id, space_id)\n",
    "print(\"User can navigate to the published facts in space {}\".format(factsheets_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have finished the hands-on lab for IBM Watson OpenScale. You can now navigate to the prompt template asset in your project / spaceand click on the Evaluate tab to visualise the results on the UI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
